{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcelra/Pycharmprojects/music-generator/venv/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, GRU, Flatten, MaxPool2D, MaxPool1D\n",
    "from keras.layers import PReLU, Dropout, Lambda, Dense\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "from music_generator.basic.random import generate_dataset\n",
    "from music_generator.basic.signalproc import SamplingInfo\n",
    "from music_generator.musical.timing import Tempo\n",
    "from music_generator.musical.scales import GenericScale\n",
    "from music_generator.analysis.play import play_mono_as_stereo, play_array\n",
    "from music_generator.basic.signalproc import mix_at\n",
    "from music_generator.analysis import preprocessing\n",
    "\n",
    "from music_generator.musical import scales\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_info = SamplingInfo(22050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_roots = scales.chromatic_scale('C')\n",
    "roots = [n.get_symbol() for n in all_roots.generate(0, 1)]\n",
    "roots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset_for_root(root):\n",
    "    return generate_dataset(n_measures=64,\n",
    "                            tempo=Tempo(120),\n",
    "                            scale=GenericScale(root, [0, 2, 3, 5, 7, 8, 10]),\n",
    "                            sampling_info=sampling_info)\n",
    "    \n",
    "with Pool(8) as pool:\n",
    "    datasets = pool.map(generate_dataset_for_root, roots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make one big data set and make sure data is of same size\n",
    "audio_tracks, mix = preprocessing.combine_datasets(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mix.shape[-1] == audio_tracks.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix = mix_at(mix_at(audio_tracks[2], audio_tracks[1]), audio_tracks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8260.6728515625"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mix) / 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10000\n",
    "fragment_length = 4096\n",
    "input_track = mix\n",
    "target_track = audio_tracks[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_array(input_track, sampling_info.sample_rate, range_secs=[50, 51])\n",
    "play_array(target_track, sampling_info.sample_rate, range_secs=[50, 51])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = preprocessing.create_training_data_set(n_samples, \n",
    "                                              fragment_length, \n",
    "                                              input_track, \n",
    "                                              target_track)\n",
    "\n",
    "x = x.reshape(x.shape + (1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_array(np.tile(x[2, :, 0], 10), sample_rate=sampling_info.sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x[0].shape\n",
    "output_shape = x[1].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fft_loss(y_true, y_pred):\n",
    "    return tf.losses.mean_squared_error(tf.abs(tf.spectral.rfft(y_true)), \n",
    "                                        tf.abs(tf.spectral.rfft(y_pred)))\n",
    "\n",
    "def mse_scaled(y_true, y_pred):\n",
    "    return 100*tf.losses.mean_squared_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x.reshape(-1, 128, 1)\n",
    "y_train = y.reshape(-1, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n_steps = 128\n",
    "batch_inp_shape = (batch_size, n_steps, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (32, 128, 1)              0         \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (32, 128)                 49920     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (32, 256)                 33024     \n",
      "_________________________________________________________________\n",
      "p_re_lu_1 (PReLU)            (32, 256)                 256       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (32, 128)                 32896     \n",
      "=================================================================\n",
      "Total params: 116,096\n",
      "Trainable params: 116,096\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = Input(batch_shape=batch_inp_shape)\n",
    "out = inp\n",
    "out = GRU(128, stateful=True)(out)\n",
    "out = Dense(256)(out)\n",
    "out = PReLU()(out)\n",
    "out = Dense(128)(out)\n",
    "model = Model(inp, out)\n",
    "model.summary()\n",
    "model.compile(Adam(lr=1e-3), mse_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 256000 samples, validate on 64000 samples\n",
      "Epoch 1/2\n",
      "256000/256000 [==============================] - 411s 2ms/step - loss: 0.4224 - val_loss: 0.2873\n",
      "Epoch 2/2\n",
      " 21248/256000 [=>............................] - ETA: 5:16 - loss: 0.2911"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=2, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_seconds_to_predict = 10\n",
    "n_batches_predict = (n_seconds_to_predict * sampling_info.sample_rate //\n",
    "                     batch_size // n_steps)\n",
    "\n",
    "pred_x = mix[0:n_batches_predict*batch_size*n_steps].reshape(-1, n_steps, 1)\n",
    "\n",
    "pred_y = model.predict(pred_x)\n",
    "play_array(pred_y, sample_rate=sampling_info.sample_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
