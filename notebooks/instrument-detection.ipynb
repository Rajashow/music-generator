{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, GRU, Flatten, MaxPool2D, MaxPool1D\n",
    "from keras.layers import PReLU, Dropout, Lambda, Dense\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "from music_generator.basic.random import generate_dataset\n",
    "from music_generator.basic.signalproc import SamplingInfo\n",
    "from music_generator.musical.timing import Tempo\n",
    "from music_generator.musical.scales import GenericScale\n",
    "from music_generator.analysis.play import play_mono_as_stereo, play_array\n",
    "from music_generator.basic.signalproc import mix_at\n",
    "from music_generator.analysis import preprocessing\n",
    "\n",
    "from music_generator.musical import scales\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "\n",
    "from scipy.io.wavfile import read\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = (16.0, 12.0)\n",
    "matplotlib.rcParams['lines.linewidth'] = 2\n",
    "matplotlib.rcParams['axes.linewidth'] = 1.5\n",
    "matplotlib.rcParams['font.size'] = 18\n",
    "matplotlib.rcParams['xtick.major.size'] = 5\n",
    "matplotlib.rcParams['xtick.major.width'] = 2\n",
    "matplotlib.rcParams['ytick.major.size'] = 5\n",
    "matplotlib.rcParams['ytick.major.width'] = 2\n",
    "matplotlib.rcParams['figure.figsize'] = (16.0, 8.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr, full_mix = read(\"../data/full-mix.wav\", mmap=False)\n",
    "sr, only_guitar = read(\"../data/only-guitar.wav\", mmap=False)\n",
    "\n",
    "full_mix = full_mix.astype(np.float) / 2**15\n",
    "only_guitar = only_guitar.astype(np.float) / 2**15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the full mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import stft\n",
    "f_vec, t_vec, Zxx = stft(full_mix, sr, nperseg=4096)\n",
    "plt.pcolormesh(t_vec, f_vec, np.abs(Zxx), vmin=0, vmax=0.5)\n",
    "# plt.title('STFT Magnitude')\n",
    "plt.ylabel('Frequency [Hz]')\n",
    "plt.xlabel('Time [sec]')\n",
    "plt.ylim(0, 3000)\n",
    "plt.xlim(30, 40);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect where guitar is playing in solo track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'guitar': only_guitar}, index=np.arange(len(only_guitar))/ 44100)\n",
    "df['full_mix'] = full_mix\n",
    "df['rolling_mse'] = df['guitar'].rolling(window=2000, center=False).std()\n",
    "df['is_playing'] = df['rolling_mse'] > 0.1\n",
    "df['is_playing_float'] = df['is_playing'] * 1.0\n",
    "df['signal_masked'] = df['is_playing'] * df['guitar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = 40\n",
    "delta = 10\n",
    "df.loc[t0:t0+delta][['guitar', 'rolling_mse', 'is_playing_float']].plot()\n",
    "plt.axhline(y=0, color='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_array(df['signal_masked'].values, range_secs=[20,30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_fragment_indices(n_batches, n_samples, fragment_length):\n",
    "    i_max = n_samples - fragment_length\n",
    "    start = np.random.randint(low=0, high=i_max, size=n_batches)\n",
    "    \n",
    "    return np.array([np.arange(s, s+fragment_length) for s in start])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df['full_mix'].values\n",
    "y_train = df['is_playing'].values\n",
    "\n",
    "fragment_length = 4096\n",
    "n_fragments = 20000\n",
    "\n",
    "ind = create_random_fragment_indices(n_fragments, len(x_train), fragment_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train[ind]\n",
    "y_train = y_train[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(-1, fragment_length)\n",
    "y_train = y_train.reshape(-1, fragment_length)\n",
    "\n",
    "y_train = np.percentile(y_train, q=80, axis=1).reshape(-1, 1)\n",
    "\n",
    "# Class balancing\n",
    "w0 = 0.5*len(x_train) / np.sum(y_train == 0) \n",
    "w1 = 0.5*len(x_train) / np.sum(y_train == 1)\n",
    "sample_weight = np.where(y_train==0, w0, w1).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "inp = Input(shape=[fragment_length])\n",
    "out = inp\n",
    "\n",
    "def apply_fft(tensor):\n",
    "    tensor = tf.cast(tensor, tf.complex64)\n",
    "    return tf.abs(tf.spectral.fft(tensor))\n",
    "\n",
    "out = Lambda(apply_fft, output_shape=[fragment_length])(out)\n",
    "out = Dense(units=1, activation='sigmoid')(out)\n",
    "\n",
    "model = Model(inp, out)\n",
    "model.summary()\n",
    "model.compile(Adam(lr=1e-3), 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs=10, validation_split=0.2, sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[0:((len(df) // fragment_length) * fragment_length)].copy()\n",
    "x_pred = df.full_mix.values.reshape(-1, fragment_length)\n",
    "y_pred = model.predict(x_pred)\n",
    "df['is_playing_prediction'] = y_pred.repeat(fragment_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import moviepy as mpy\n",
    "from moviepy.editor import VideoClip, AudioFileClip, clips_array\n",
    "from moviepy.video.io.bindings import mplfig_to_npimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = (8.0, 4.0)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "def make_frame1(t):\n",
    "    ax.clear()\n",
    "    ix = df.index.searchsorted(t)\n",
    "    c = '0.5' if df.iloc[ix]['is_playing_prediction'] < 0.5 else 'b'\n",
    "    ax.plot(df.loc[t-0.2:t]['full_mix'].values, color=c)\n",
    "    ax.set_ylim(-2, 2)\n",
    "    return mplfig_to_npimage(fig)\n",
    "\n",
    "def make_frame2(t):\n",
    "    ax.clear()\n",
    "    ix = df.index.searchsorted(t)\n",
    "    c = '0.5' if df.iloc[ix]['is_playing_prediction'] < 0.5 else 'b'\n",
    "    ax.plot(df.loc[t-0.2:t]['guitar'].values, color=c)\n",
    "    ax.set_ylim(-2, 2)\n",
    "    return mplfig_to_npimage(fig)\n",
    "\n",
    "def make_frame3(t):\n",
    "    ax.clear()\n",
    "    ax.plot(df.loc[t-1:t]['is_playing'].values)\n",
    "    ax.plot(df.loc[t-1:t]['is_playing_prediction'].values)\n",
    "    ax.set_ylim(-0.5, 1.5)\n",
    "    return mplfig_to_npimage(fig)\n",
    "\n",
    "duration = df.index.max()\n",
    "# duration = 10\n",
    "\n",
    "clip1 = VideoClip(make_frame1, duration=duration)\n",
    "clip2 = VideoClip(make_frame2, duration=duration)\n",
    "clip3 = VideoClip(make_frame3, duration=duration)\n",
    "\n",
    "audio = AudioFileClip(\"../data/full-mix.wav\", fps=44100)\n",
    "clip1.audio = audio\n",
    "\n",
    "final_clip = clips_array([[clip1, clip2], [clip3, clip3]])\n",
    "# final_clip.ipython_display(fps=20, autoplay=True, maxduration=300)\n",
    "final_clip.to_videofile(\"detection.mp4\", fps=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
