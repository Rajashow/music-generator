{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from music_generator.basic.random import generate_dataset\n",
    "from music_generator.basic.signalproc import SamplingInfo\n",
    "from music_generator.musical.timing import Tempo\n",
    "from music_generator.musical.scales import GenericScale\n",
    "from music_generator.analysis.play import play_mono_as_stereo, play_array\n",
    "from music_generator.basic.signalproc import mix_at\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_info = SamplingInfo(44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = generate_dataset(n_measures=64,\n",
    "                       tempo=Tempo(120),\n",
    "                       scale=GenericScale('E', [0, 2, 3, 5, 7, 8, 10]),\n",
    "                       sampling_info=sampling_info)\n",
    "\n",
    "ds2 = generate_dataset(n_measures=64,\n",
    "                       tempo=Tempo(120),\n",
    "                       scale=GenericScale('F#', [0, 2, 3, 5, 7, 8, 10]),\n",
    "                       sampling_info=sampling_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from music_generator.analysis.preprocessing import combine_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def combine_datasets(ds1, ds2):\n",
    "    \n",
    "#     print(\"WARNING: not combining the score tracks\")\n",
    "#     audio_tracks = [np.concatenate((x1, x2)) for x1, x2 in zip(ds1[1], ds2[1])]\n",
    "#     mix = np.concatenate((ds1[2], ds2[2]))\n",
    "    \n",
    "#     return audio_tracks, mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_tracks, mix = combine_datasets(ds1, ds2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_mono_as_stereo(mix[0:5*44100], sampling_info.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play_mono_as_stereo(mix_at(mix, -audio_tracks[2], at=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 4096 * 10\n",
    "fragment_length = 1024 * 1\n",
    "input_track = mix\n",
    "target_track = audio_tracks[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from music_generator.analysis.preprocessing import create_training_data_set\n",
    "# def create_training_data_set(n_samples, fragment_length, input_track, target_track):\n",
    "#     max_index = min(len(mix), len(target_track))\n",
    "#     max_start_index = max_index - fragment_length\n",
    "    \n",
    "#     # Selection range\n",
    "#     selection_ranges = np.random.randint(0, max_start_index, n_samples)\n",
    "#     selection_ranges = [{'begin': x, 'end': x + fragment_length} for x in selection_ranges]\n",
    "    \n",
    "#     x = np.array([mix[sr['begin']:sr['end']] for sr in selection_ranges])\n",
    "#     y = np.array([target_track[sr['begin']:sr['end']] for sr in selection_ranges])\n",
    "    \n",
    "#     return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = create_training_data_set(n_samples, fragment_length, mix, audio_tracks[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_fade_profile(batch_dim):\n",
    "    x = np.arange(batch_dim)\n",
    "    return 1 - abs(x - (batch_dim / 2)) / (batch_dim / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(model, input_track):\n",
    "    dim = input_shape[0]\n",
    "    n_batches = int(len(input_track) / dim) - 1\n",
    "    pred_batches = input_track[0:n_batches*dim].reshape((-1, dim))\n",
    "    \n",
    "    pred_batches_shifted = input_track[dim//2:n_batches*dim + dim//2].reshape((-1, dim))\n",
    "    \n",
    "    xfp = x_fade_profile(dim)\n",
    "    \n",
    "    x0 = np.array([xfp * batch for batch in model.predict(pred_batches)]).reshape(-1)\n",
    "    x1 = np.array([xfp * batch for batch in model.predict(pred_batches_shifted)]).reshape(-1)\n",
    "    \n",
    "    return mix_at(x0, x1, dim//2)\n",
    "#     np.tanh()\n",
    "\n",
    "# play_array(model_predict(model, mix), do_wait_done=False)\n",
    "# play_array(model_predict(model, mix[0:5*44100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, PReLU\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x[0].shape\n",
    "output_shape = x[1].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(Dense(1024, input_shape=input_shape))\n",
    "model.add(PReLU())\n",
    "model.add(Dense(256))\n",
    "model.add(PReLU())\n",
    "model.add(Dense(output_shape))\n",
    "model.compile(Adam(), 'mse')\n",
    "play_array(model_predict(model, mix)[0:5*44100], do_wait_done=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    model.fit(x, y, epochs=2)\n",
    "    play_array(model_predict(model, mix)[0:5*44100], do_wait_done=True)  \n",
    "    play_array(mix[0:5*44100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_array(model_predict(model, mix[0:15*44100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_array(mix[0:15*44100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io.wavfile import read\n",
    "wf = read(\"/Users/marcelraas/dev/test_data/anotherDay.wav\", mmap=False)\n",
    "# play_array(model_predict(model, wf[1][120*44100:240*44100, 0] / 32000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_tracks_t, audio_tracks_t, mix_t = generate_dataset(n_measures=64,\n",
    "                                                         tempo=Tempo(120),\n",
    "                                                         scale=GenericScale('E', [0, 1, 4, 5, 7, 8, 10]),\n",
    "                                                         sampling_info=sampling_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_array(model_predict(model, mix_t[0:15*44100]))\n",
    "# play_array(model_predict(model, audio_tracks_t[2][0:15*44100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
